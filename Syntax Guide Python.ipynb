{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2a3d0afd-55ca-4579-a74a-966502b59a51",
   "metadata": {},
   "source": [
    "# ML/AI Python Syntax"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2efc01f7-bb33-4cb5-bbf9-d73a4739e72f",
   "metadata": {},
   "source": [
    "### Insert Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6596b1e-3b08-4011-b407-f2ca1e750f53",
   "metadata": {},
   "outputs": [],
   "source": [
    "from Ipython.display import Image\n",
    "Image(\"Desktop\\AI.jpg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d42c28a5-71a4-4bef-8cf8-41477912d536",
   "metadata": {},
   "source": [
    "[Markdown Syntax for Jupyter Notebook](https://www.google.com/search?sca_esv=bf3933bb65ad3238&rlz=1C1VDKB_enIN1116IN1116&sxsrf=ADLYWIJi5feXrQaLkoCEOl4naiF2QKwgTg:1731563028360&q=how+to+write+text+in+jupyter+notebook&udm=7&fbs=AEQNm0Aa4sjWe7Rqy32pFwRj0UkWwAFG7ranuZ26H8lR7pf_8AzBs6lnFFuPH6eU3OV27QKh6ftn9lc4yAcaBgSvqjbS08AwYK5ArknIAZUHpOTMwOfbDzKV1Lg2Z-eWD1L9zdnHdJ4sj2t_Q5kwi_HNvC8EgEhQLOg6AHB7Se-IQZO7CfiZgJ_RkuJCJFoIxEJeBEul6X5BiaUsuxkiVfXWoRDLX9ufwA&sa=X&ved=2ahUKEwjj-9f5jtuJAxVV-zgGHcLgJbcQtKgLegQIEhAB&biw=1280&bih=631&dpr=1.5#fpstate=ive&vld=cid:ccdf81a7,vid:mTIifW_LU5s,st:0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31ad7c04-7b59-4a62-b623-d95599ad5b55",
   "metadata": {},
   "source": [
    "#### Uploading Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14c5080c-8514-4a06-a78e-0b5a7fc5a9c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_samples, silhouette_score\n",
    "from sklearn import preprocessing, model_selection, metrics, feature_selection\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV, learning_curve\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn import neighbors, linear_model, svm, tree, ensemble\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31090f16-285e-474d-85db-11e1a8928ea3",
   "metadata": {},
   "source": [
    "##### Importing the file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8aa73f7-1f9f-4ad5-a054-b68bb176407b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"laptop_data.csv\")\n",
    "df.info()\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39f79e8f-f280-4a3d-8984-a776fa21393a",
   "metadata": {},
   "source": [
    "### Gives you a breakdown of your data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "264fcf9f-0904-4dc3-bce7-96c768cbbb5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Give some info on columns types and number of null values\n",
    "tab_info = pd.DataFrame(df_initial.dtypes).T.rename(index={0: 'column type'})\n",
    "\n",
    "# Use pd.concat() instead of append()\n",
    "tab_info = pd.concat([tab_info, \n",
    "                      pd.DataFrame(df_initial.isnull().sum()).T.rename(index={0: 'null values (nb)'}),\n",
    "                      pd.DataFrame(df_initial.isnull().sum() / df_initial.shape[0] * 100).T.rename(index={0: 'null values (%)'})])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70f2f189-8c20-46e0-9f41-c73e758349cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame([{\n",
    "    'products': df_initial['StockCode'].nunique(),\n",
    "    'transactions': df_initial['InvoiceNo'].nunique(),\n",
    "    'customers': df_initial['CustomerID'].nunique()\n",
    "}], columns=['products', 'transactions', 'customers'], index=['quantity'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d296dbd9-041a-4428-a0cd-57247b1e5ade",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This line of code removes any rows from df_initial where the CustomerID column has missing (NaN) values:\n",
    "df_initial.dropna(axis=0, subset=['CustomerID'], inplace=True)\n",
    "#1) dropna(axis=0): Drops rows (axis=0) with missing values.\n",
    "#2) subset=['CustomerID']: Specifies that only rows with missing values in the CustomerID column should be dropped.\n",
    "#3) inplace=True: Modifies df_initial directly rather than creating a new DataFrame."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94cde17a-833a-4669-aedc-f535630a43f5",
   "metadata": {},
   "source": [
    "### Create a list of numeric and categorical columns  separately"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3713e5d-7c75-4f5d-a223-4cb87939b28a",
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_columns = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "numeric_df = df[numeric_columns]\n",
    "\n",
    "categorical_columns = df.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "categorical_df = df[categorical_columns]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "927ac907-98ef-41db-bc42-f14a049fd821",
   "metadata": {},
   "source": [
    "## Converting the Target column from string to numeric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7a46788-0633-4e97-91c4-7d86b1b0c402",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "encoder = LabelEncoder()\n",
    "df['target'] = encoder.fit_transform(df['target'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5174b7db-f624-4050-b467-c2596bb95144",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.duplicated().sum()\n",
    "df.isnull().sum()\n",
    "\n",
    "# remove duplicates\n",
    "df = df.drop_duplicates(keep='first')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "eb9535c1-e145-4535-97b7-334b9936e95f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#converting to date time\n",
    "df_initial['InvoiceDate'] = pd.to_datetime(df_initial['InvoiceDate'])\n",
    "# renaming the cols\n",
    "df.rename(columns={'v1':'target','v2':'text'},inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d93eb693-8a64-4d5b-b248-1548ab3ceb08",
   "metadata": {},
   "source": [
    "## Binning the Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58b36fc7-5f3d-412c-b9bf-adf8fee64da2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pd.cut() function can be used to categorize the Basket Price values into discrete bins based on the price_range interval\n",
    "import pandas as pd\n",
    "\n",
    "# Define price ranges\n",
    "price_range = [0, 50, 100, 200, 500, 1000, 5000, 50000]\n",
    "\n",
    "# Use pd.cut to categorize Basket Price into bins\n",
    "basket_price['price_category'] = pd.cut(basket_price['Basket Price'], bins=price_range)\n",
    "\n",
    "# Count the number of values in each bin\n",
    "count_price = basket_price['price_category'].value_counts().sort_index()\n",
    "\n",
    "# Convert to a list if needed\n",
    "count_price = count_price.tolist()\n",
    "\n",
    "print(count_price)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eedbe6f1-32dd-4bd8-9b47-6914ea6b1f9b",
   "metadata": {},
   "source": [
    "### Manual Function to do it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e14e5fcb-781f-4427-afb8-93d55cb0ca83",
   "metadata": {},
   "outputs": [],
   "source": [
    "price_range = [0, 50, 100, 200, 500, 1000, 5000, 50000]\n",
    "count_price = []\n",
    "for i, price in enumerate(price_range):\n",
    "    if i == 0: continue\n",
    "    val = basket_price[(basket_price['Basket Price'] < price) &\n",
    "                       (basket_price['Basket Price'] > price_range[i-1])]['Basket Price'].count()\n",
    "    count_price.append(val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a4506ee1-8e2b-461d-bcf2-b598a115bf8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming you have certain values like - and special charecters in your data you want to replace with random values in the same column \n",
    "# Values to omit\n",
    "import numpy as np\n",
    "\n",
    "# Define the unsupported values\n",
    "value_omitted = ('â€“', 'not supported')\n",
    "\n",
    "# Get the valid values for fuel type\n",
    "value_req = df[~df['fuel_type'].isin(value_omitted)]['fuel_type']\n",
    "\n",
    "# Replace unsupported values with random choices from the valid values\n",
    "df['fuel_type'] = df['fuel_type'].apply(lambda x: np.random.choice(value_req) if x in value_omitted else x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f836c513-4dc0-4df9-ad8a-68cb419c81bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# you want tot remove certain charecters from your column \n",
    "df['Ram'] = df['Ram'].str.replace('GB','')\n",
    "df['Ram'] = df['Ram'].astype('int32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0a25a2d6-21b8-40e6-8dfd-25478d2c6a56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do a quick plot\n",
    "sns.barplot(x=df['Company'],y=df['Price'])\n",
    "plt.xticks(rotation='vertical')\n",
    "plt.show()\n",
    "\n",
    "#Do a frequency Plot \n",
    "df['Company'].value_counts().plot(kind='bar')\n",
    "\n",
    "#Scatter Plot\n",
    "sns.scatterplot(x=df['Weight'],y=df['Price'])\n",
    "\n",
    "#Pie Chart\n",
    "import matplotlib.pyplot as plt\n",
    "plt.pie(df['target'].value_counts(), labels=['ham','spam'],autopct=\"%0.2f\")\n",
    "plt.show()\n",
    "\n",
    "#PairPlot\n",
    "sns.pairplot(df,hue='target')\n",
    "\n",
    "#HeatMap\n",
    "sns.heatmap(df.corr(),annot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "644b74bd-8b79-4f87-96b2-d49212a10408",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assume you want to convert a category column to a binary column based on a condition if the column value contains a particular string\n",
    "df['Touchscreen']=df['ScreenResolution'].apply( lambda x:1 if 'Touchscreen' in x else 0)\n",
    "\n",
    "#Assume you have column 'ScreenResolution' that has values like 'IPS Panel Touchscreen 1920x1200', you want \n",
    "# str.split('x', n=1) splits each string in the ScreenResolution column at the first occurrence of 'x'\n",
    "#.expand=True: Expands the split components into separate columns\n",
    "new=df['ScreenResolution'].str.split('x',n=1,expand=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad7b4ccc-869a-4153-affe-251f71a4ed13",
   "metadata": {},
   "outputs": [],
   "source": [
    "#To drop any column\n",
    "df.drop(columns=['Resolution'], inplace=True)\n",
    "\n",
    "#Lets say you have column called 'Cpu' that has values like \"Intel Core i7 8550U\",\"AMD Ryzen 5 3500U\" and you want to extract only the first three words\n",
    "df['Cpu_Model'] = df['Cpu'].apply(lambda x: \" \".join(x.split()[0:3]))\n",
    "#1) x.split(): Splits the string x (each value in the Cpu column) by whitespace, creating a list of words.\n",
    "#2) [0:3]: Takes the first three elements from this list (i.e., the first three words).\n",
    "#3) \" \".join(...): Joins these first three words back into a single string with spaces in between."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fe1b07e-56de-4ecc-88d0-99d0cecd8e64",
   "metadata": {},
   "source": [
    "Without performing hyperparameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a529c54-67a0-4693-adc3-ffbaa70493f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "svc = SVC(kernel='sigmoid', gamma=1.0)\n",
    "knc = KNeighborsClassifier()\n",
    "mnb = MultinomialNB()\n",
    "dtc = DecisionTreeClassifier(max_depth=5)\n",
    "lrc = LogisticRegression(solver='liblinear', penalty='l1')\n",
    "rfc = RandomForestClassifier(n_estimators=50, random_state=2)\n",
    "abc = AdaBoostClassifier(n_estimators=50, random_state=2)\n",
    "bc = BaggingClassifier(n_estimators=50, random_state=2)\n",
    "etc = ExtraTreesClassifier(n_estimators=50, random_state=2)\n",
    "gbdt = GradientBoostingClassifier(n_estimators=50,random_state=2)\n",
    "xgb = XGBClassifier(n_estimators=50,random_state=2)\n",
    "\n",
    "clfs = {\n",
    "    'SVC' : svc,\n",
    "    'KN' : knc, \n",
    "    'NB': mnb, \n",
    "    'DT': dtc, \n",
    "    'LR': lrc, \n",
    "    'RF': rfc, \n",
    "    'AdaBoost': abc, \n",
    "    'BgC': bc, \n",
    "    'ETC': etc,\n",
    "    'GBDT':gbdt,\n",
    "    'xgb':xgb\n",
    "}\n",
    "\n",
    "def train_classifier(clf,X_train,y_train,X_test,y_test):\n",
    "    clf.fit(X_train,y_train)\n",
    "    y_pred = clf.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test,y_pred)\n",
    "    precision = precision_score(y_test,y_pred)\n",
    "    \n",
    "    return accuracy,precision\n",
    "\n",
    "# for seeing on a single use \n",
    "classifiertrain_classifier(svc,X_train,y_train,X_test,y_test)\n",
    "\n",
    "\n",
    "# For seeing the model performance across all the models\n",
    "accuracy_scores = []\n",
    "precision_scores = []\n",
    "\n",
    "for name,clf in clfs.items():\n",
    "    \n",
    "    current_accuracy,current_precision = train_classifier(clf, X_train,y_train,X_test,y_test)\n",
    "    \n",
    "    print(\"For \",name)\n",
    "    print(\"Accuracy - \",current_accuracy)\n",
    "    print(\"Precision - \",current_precision)\n",
    "    \n",
    "    accuracy_scores.append(current_accuracy)\n",
    "    precision_scores.append(current_precision)\n",
    "\n",
    "#view it on a dataframe\n",
    "performance_df = pd.DataFrame({'Algorithm':clfs.keys(),'Accuracy':accuracy_scores,'Precision':precision_scores}).sort_values('Precision',ascending=False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "150df298-9762-4274-9d3f-91c7c377c3e7",
   "metadata": {},
   "source": [
    "## Pipeline preprocessing and Fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "befcd264-fc9a-4808-a94d-6f7806db9adb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Linear Rigression\n",
    "\n",
    "\n",
    "company_order = ['Apple', 'HP', 'Dell', 'Lenovo', 'Acer']\n",
    "cpu_brand_order = ['Intel Core i7', 'Intel Core i5' , 'Intel Core i3', 'AMD Processor', 'Other Intel Processor']\n",
    "gpu_brand_order = ['Intel', 'Nvidia', 'AMD']\n",
    "os_order = ['Mac', 'Windows', 'Linux', 'Others']\n",
    "\n",
    "# Step 1: Column Transformer with both OneHotEncoder and OrdinalEncoder, specifying categories order\n",
    "step1 = ColumnTransformer(transformers=[\n",
    "    ('ord_enc', OrdinalEncoder(categories=[cpu_brand_order, gpu_brand_order, os_order]), \n",
    "     ['CPU_Brand', 'Gpu_brand', 'os']),  # Ordinal Encoding with custom order with the required column names\n",
    "    ('ohe', OneHotEncoder(drop='first'), ['TypeName','Company'])  # OneHotEncoding for 'TypeName'\n",
    "], remainder='passthrough')  # Pass through the rest of the columns (which are numerical)\n",
    "\n",
    "# Step 2: Polynomial Features transformation\n",
    "step2 = PolynomialFeatures(degree=2, include_bias=True)  # Creating interaction terms and polynomial terms\n",
    "\n",
    "# Step 3: RobustScaler for numerical features\n",
    "step3 = RobustScaler()\n",
    "\n",
    "# Step 4: Linear Regression Model (for polynomial regression)\n",
    "step4 = LinearRegression()\n",
    "\n",
    "\n",
    "# Create the pipeline with polynomial features and linear regression\n",
    "pipe = Pipeline([\n",
    "    ('step1', step1),        # Categorical encoding\n",
    "    ('step2', step2),        # Polynomial transformation\n",
    "    ('step3', step3),        # Standardization\n",
    "    ('step4', step4)         # Linear Regression model (for polynomial regression)\n",
    "])\n",
    "\n",
    "pipe.fit(X_train,y_train)\n",
    "\n",
    "y_pred = pipe.predict(X_test)\n",
    "\n",
    "print('R2 score',r2_score(y_test,y_pred))\n",
    "print('MAE',mean_absolute_error(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbccea05-6e20-4027-a217-b64ee55afceb",
   "metadata": {},
   "source": [
    "###  Another key function to learn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d536542a-8cb2-422e-8c84-17d33118f174",
   "metadata": {},
   "source": [
    "## Pre Processing in NLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86296351-3ad7-48ce-a9f5-19f0d769d4e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_text(text):\n",
    "    text = text.lower()\n",
    "    text = nltk.word_tokenize(text)\n",
    "    \n",
    "    y = []\n",
    "    for i in text:\n",
    "        if i.isalnum():\n",
    "            y.append(i)\n",
    "    \n",
    "    text = y[:]\n",
    "    y.clear()\n",
    "    \n",
    "    for i in text:\n",
    "        if i not in stopwords.words('english') and i not in string.punctuation:\n",
    "            y.append(i)\n",
    "            \n",
    "    text = y[:]\n",
    "    y.clear()\n",
    "    \n",
    "    for i in text:\n",
    "        y.append(ps.stem(i))\n",
    "    \n",
    "            \n",
    "    return \" \".join(y)\n",
    "    df['transformed_text'] = df['text'].apply(transform_text)\n",
    "\n",
    "from wordcloud import WordCloud\n",
    "wc = WordCloud(width=500,height=500,min_font_size=10,background_color='white')\n",
    "spam_wc = wc.generate(df[df['target'] == 1]['transformed_text'].str.cat(sep=\" \")\n",
    "plt.figure(figsize=(15,6))\n",
    "plt.imshow(spam_wc)\n",
    "\n",
    "spam_corpus = []\n",
    "for msg in df[df['target'] == 1]['transformed_text'].tolist():\n",
    "    for word in msg.split():\n",
    "        spam_corpus.append(word)\n",
    "\n",
    "from collections import Counter\n",
    "sns.barplot(pd.DataFrame(Counter(spam_corpus).most_common(30))[0],pd.DataFrame(Counter(spam_corpus).most_common(30))[1])\n",
    "plt.xticks(rotation='vertical')\n",
    "plt.show()\n",
    "\n",
    "#Model Building\n",
    "from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\n",
    "cv = CountVectorizer()\n",
    "tfidf = TfidfVectorizer(max_features=3000)\n",
    "X = tfidf.fit_transform(df['transformed_text']).toarray()\n",
    "y = df['target'].values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1c240aa-2ff5-4d5d-800a-f40ca0f92566",
   "metadata": {},
   "outputs": [],
   "source": [
    "def keywords_inventory(dataframe, colonne = 'Description'):\n",
    "    stemmer = nltk.stem.SnowballStemmer(\"english\")\n",
    "    keywords_roots  = dict()  # collect the words / root\n",
    "    keywords_select = dict()  # association: root <-> keyword\n",
    "    category_keys   = []\n",
    "    count_keywords  = dict()\n",
    "    icount = 0\n",
    "    for s in dataframe[colonne]:\n",
    "        if pd.isnull(s): continue\n",
    "        lines = s.lower()\n",
    "        tokenized = nltk.word_tokenize(lines)\n",
    "        nouns = [word for (word, pos) in nltk.pos_tag(tokenized) if is_noun(pos)] \n",
    "        \n",
    "        for t in nouns:\n",
    "            t = t.lower() ; racine = stemmer.stem(t)\n",
    "            if racine in keywords_roots:                \n",
    "                keywords_roots[racine].add(t)\n",
    "                count_keywords[racine] += 1                \n",
    "            else:\n",
    "                keywords_roots[racine] = {t}\n",
    "                count_keywords[racine] = 1\n",
    "    \n",
    "    for s in keywords_roots.keys():\n",
    "        if len(keywords_roots[s]) > 1:  \n",
    "            min_length = 1000\n",
    "            for k in keywords_roots[s]:\n",
    "                if len(k) < min_length:\n",
    "                    clef = k ; min_length = len(k)            \n",
    "            category_keys.append(clef)\n",
    "            keywords_select[s] = clef\n",
    "        else:\n",
    "            category_keys.append(list(keywords_roots[s])[0])\n",
    "            keywords_select[s] = list(keywords_roots[s])[0]\n",
    "                   \n",
    "    print(\"Nb of keywords in variable '{}': {}\".format(colonne,len(category_keys)))\n",
    "    return category_keys, keywords_roots, keywords_select, count_keywords"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dc7154e-bb9e-4644-b703-adfbff9da942",
   "metadata": {},
   "source": [
    "## Random data preprocessing function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b414e04-d88c-44be-be3d-d44156be93a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_cleaned = df_initial.copy(deep = True)\n",
    "df_cleaned['QuantityCanceled'] = 0\n",
    "\n",
    "entry_to_remove = [] ; doubtfull_entry = []\n",
    "\n",
    "for index, col in  df_initial.iterrows():\n",
    "    if (col['Quantity'] > 0) or col['Description'] == 'Discount': continue        \n",
    "    df_test = df_initial[(df_initial['CustomerID'] == col['CustomerID']) &\n",
    "                         (df_initial['StockCode']  == col['StockCode']) & \n",
    "                         (df_initial['InvoiceDate'] < col['InvoiceDate']) & \n",
    "                         (df_initial['Quantity']   > 0)].copy()\n",
    "    #___________________________________\n",
    "    # Cancelation WITHOUT counterpart\n",
    "    if (df_test.shape[0] == 0): \n",
    "        doubtfull_entry.append(index)\n",
    "    #________________________________\n",
    "    # Cancelation WITH a counterpart\n",
    "    elif (df_test.shape[0] == 1): \n",
    "        index_order = df_test.index[0]\n",
    "        df_cleaned.loc[index_order, 'QuantityCanceled'] = -col['Quantity']\n",
    "        entry_to_remove.append(index)        \n",
    "    #______________________________________________________________\n",
    "    # Various counterparts exist in orders: we delete the last one\n",
    "    elif (df_test.shape[0] > 1): \n",
    "        df_test.sort_index(axis=0 ,ascending=False, inplace = True)        \n",
    "        for ind, val in df_test.iterrows():\n",
    "            if val['Quantity'] < -col['Quantity']: continue\n",
    "            df_cleaned.loc[ind, 'QuantityCanceled'] = -col['Quantity']\n",
    "            entry_to_remove.append(index) \n",
    "            break            "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
